{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\AAI47\\chatbot_poc1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import faiss\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader\n",
    "import re\n",
    "import requests\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os \n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding and Reranking API Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_api_endpoint = 'http://px101.prod.exalead.com:8110/v1/embeddings'\n",
    "reranking_api_endpoint = 'http://px101.prod.exalead.com:8110/v1/rerank'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page-Level parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"4/3/24, 10:34 AM Accessing the 3DSearch Service - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-t-Search-access.htm?contextscope=cloud&id=dc4790a5d0f44c4b… 1/2\\n© 1995-2024  Dassault Systèmes. All rights reserved.\\nAccessing the 3DSearch Service\\nYou can access the 3DSearch service from the 3DEXPERIENCE platform Top Bar or the\\n3DSearch dashboard app.\\n1. Access 3DSearch by doing one of the following:\\nUse the search field in the 3DEXPERIENCE platform Top Bar.\\nOpen the 3DSearch dashboard app from the Compass.\\nThe 3DSearch app is particularly useful if you are using CATIAV5 POWER'BY. It gives you a quick access\\nto search from the 3DEXPERIENCE platform side panel, since native apps do not display the Top Bar.\\nOnce the 3DSearch panel appears, you can dock it anywhere on the frame. It remains pinned on the\\nselected side, even if you open or explore your search results.\", \"4/3/24, 10:34 AM Accessing the 3DSearch Service - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-t-Search-access.htm?contextscope=cloud&id=dc4790a5d0f44c4b… 2/2\\nNote: In the widget \\ue115 Preferences, you can:\\nChange the platform tenant.\\nChange the widget's default name.\\n2. Optional: The 6W Search experience is the default search mode. To access additional search modes and\\noptions, click \\ue139.\\nNote: By default, the search field displays a Search placeholder. When you select another mode, this\\nplaceholder changes so that you always know which is the current mode.\"]\n"
     ]
    }
   ],
   "source": [
    "# Parse the PDF into a list of strings, one for each page.\n",
    "# def parse_pdf(file_content, filename):\n",
    "#     pdf = PdfReader(BytesIO(file_content))\n",
    "#     text_pages = []\n",
    "#     for page in pdf.pages:\n",
    "#         text = page.extract_text() \n",
    "#         text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text) #removes hyphenation at line breaks For example, \"inter-\\nnational\" becomes \"international\".\n",
    "#         text = re.sub(r'\\n\\s*\\n', '\\n\\n', text.strip()) # cleans up the whitespace, ensuring that paragraphs are separated by a single blank line\n",
    "#         text_pages.append(text)\n",
    "#     return text_pages, filename\n",
    "# #test\n",
    "# pdf_content = open('Accessing the 3DSearch Service - 2024x - dsdoc.pdf', 'rb').read()\n",
    "# text_pages, _ = parse_pdf(pdf_content,\"Accessing the 3DSearch Service - 2024x - dsdoc.pdf\")\n",
    "# print(text_pages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\AAI47\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\AAI47\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\AAI47\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'https?://\\S+|www\\.\\S+', '', sentence)\n",
    "    sentence = re.sub(r'<.*?>', '', sentence)\n",
    "    sentence = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    sentence = re.sub(r'\\d+', '', sentence)\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "    tokens = word_tokenize(sentence)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    preprocessed_sentence = ' '.join(tokens)\n",
    "    return preprocessed_sentence\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Embeddings using transformers\n",
    "\n",
    "TODO\n",
    "\n",
    "Next Using Sentence Transformers for semantic search (cossim)\n",
    "https://huggingface.co/spaces/sentence-transformers/embeddings-semantic-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')  \n",
    "\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "# all-MiniLM-L6-v2 model from the Sentence Transformers library produces embeddings with a dimensionality of 384."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating Embeddings using transformers\n",
    "sentence level, parse one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4/8/24, 11:17 AM Filtering with 6WTags - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-m-Filtering-with-6WTags-sb.htm?contextscope=cloud&id=b824255… 1/16\\n© 1995-2024  Dassault Systèmes.', 'All rights reserved.', 'Filtering with 6WTags\\n6WTags reveal information through 6W categories, and can allow you to filter content in a natural\\nway.', '6WTags can display up to 6 W categories:\\n\\ue203 What, \\ue201 Who, \\ue202 When, \\ue204 Where, \\ue206 How, \\ue205 Why.', 'In this section:\\nAbout 6WTags\\nUsing 6WTags\\nUsing User-Defined Tags\\nApp-Specific Integrations\\nAbout 6WTags\\n6WTags can display 6 W categories:\\n\\ue203 What, \\ue201 Who, \\ue202 When, \\ue204 Where, \\ue206 How, \\ue205 Why.', 'Almost everything on the 3DEXPERIENCE platform can use 6WTags to make it\\neasier to organize the content stored there.', 'This page discusses:\\n6WTags Used with 3DEXPERIENCE platform Services\\n6WTags Used in App Widgets\\nAutomatically Created or User-Defined Tags\\n6WTags Used with 3DEXPERIENCE platform Services\\n6WTags natively interacts with other 3DEXPERIENCE platform Services.', 'For example, for the\\n3DSwym service, you can use:\\nThe \\ue203 What tag to filter on the type of content that is Posts, Ideas, Questions, etc.', '4/8/24, 11:17 AM Filtering with 6WTags - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-m-Filtering-with-6WTags-sb.htm?contextscope=cloud&id=b824255… 2/16The \\ue201 Who tag to see the persons who intervened on the content.', 'The \\ue202 When tag to focus on a specific date range.']\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(file_content):\n",
    "    pdf = PdfReader(BytesIO(file_content))\n",
    "    sentences = []\n",
    "    for page in pdf.pages:\n",
    "        text = page.extract_text()\n",
    "        text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)  # Remove hyphenation\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text.strip())  # Clean whitespace\n",
    "        page_sentences = sent_tokenize(text)  # Tokenize text into sentences\n",
    "        sentences.extend(page_sentences)  # Add sentences to the list\n",
    "    return sentences\n",
    "\n",
    "# Test the function\n",
    "pdf_content = open('User Assisyance R2024x\\Filtering with 6WTags - 2024x - dsdoc.pdf', 'rb').read()\n",
    "sentences = parse_pdf(pdf_content)\n",
    "print(sentences[:10]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding with BAAI/bge-large-en-v1.5 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01225891 -0.00537589  0.0040706  ... -0.026321    0.02957748\n",
      "   0.0481206 ]]\n",
      "(1, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_embeddings(text_chunks):\n",
    "    api_key = \"vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=\"\n",
    "    embeddings_url = \"http://px101.prod.exalead.com:8110/v1/embeddings\"\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    payload = {\n",
    "    \"model\": \"BAAI/bge-large-en-v1.5\",\n",
    "    \"input\": text_chunks,\n",
    "    \"encoding_format\": \"float\",\n",
    "    \"instruct\": \"string\" ,\n",
    "    \"model\": \"BAAI/bge-large-en-v1.5\",\n",
    "}\n",
    "    response = requests.post(embeddings_url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # return response.json()\n",
    "        # the build_faiss_index function expects a NumPy array (or something similar that has a shape attribute, like a PyTorch tensor).\n",
    "        response_data = response.json()\n",
    "        embeddings_list = []\n",
    "    \n",
    "        for item in response_data['data']:\n",
    "            embeddings_list.append(item['embedding'])\n",
    "        \n",
    "        embeddings_array = np.array(embeddings_list)\n",
    "\n",
    "        return embeddings_array\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f\"Failed to get embeddings: {response.status_code}, {response.text}\")\n",
    "\n",
    "# Step : this function is used to get embeddings for the pdf \n",
    "# Step 2: this function is used also to get embeddings for the user's query\n",
    "\n",
    "#test\n",
    "text_to_embed = \"What is the capital of France?\"\n",
    "embeddings_response = get_embeddings(text_to_embed)\n",
    "print(embeddings_response)\n",
    "print(embeddings_response.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parsing and embedding all the pdf corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02371418 -0.01170784  0.06592864 ... -0.03024988  0.06783566\n",
      "  -0.01167751]\n",
      " [-0.01149366  0.02972113 -0.07571463 ...  0.03092059 -0.06258635\n",
      "   0.01264314]\n",
      " [ 0.00608097  0.08616594  0.00547728 ... -0.09285986  0.0636213\n",
      "   0.05729565]\n",
      " ...\n",
      " [ 0.03509654  0.01482087  0.04644598 ...  0.02865656  0.01619213\n",
      "   0.03748186]\n",
      " [ 0.06720083  0.03524955  0.05275492 ... -0.02248411  0.02259353\n",
      "   0.07767147]\n",
      " [ 0.10980979  0.01822141  0.08100167 ...  0.05859391  0.0118503\n",
      "   0.05559069]]\n",
      "shape (996, 384)\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "\n",
    "# def process_corpus(pdf_directory):\n",
    "#     all_embeddings = [] \n",
    "\n",
    "#     for pdf_file in os.listdir(pdf_directory):\n",
    "#         if pdf_file.endswith('.pdf'):\n",
    "#             pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "#             with open(pdf_path, 'rb') as f:  \n",
    "#                 pdf_content = f.read()\n",
    "            \n",
    "#             sentences = parse_pdf(pdf_content)\n",
    "            \n",
    "#             for sentence in sentences:\n",
    "#                 preprocessed_sentence = preprocess_text(sentence)\n",
    "#                 embedding = generate_embedding(preprocessed_sentence)\n",
    "#                 all_embeddings.append(embedding) \n",
    "#     embeddings_array = np.array(all_embeddings)\n",
    "#     return embeddings_array\n",
    "\n",
    "\n",
    "# pdf_directory = 'User Assisyance R2024x'\n",
    "# corpus_embeddings=process_corpus(pdf_directory)\n",
    "# print(corpus_embeddings)\n",
    "# print(\"shape\",corpus_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of corpus embeddings (996, 384)\n",
      "len sentences 996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_corpus(pdf_directory):\n",
    "    all_embeddings = [] \n",
    "    all_sentences=[]\n",
    "\n",
    "    for pdf_file in os.listdir(pdf_directory):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            with open(pdf_path, 'rb') as f:  \n",
    "                pdf_content = f.read()\n",
    "            \n",
    "            sentences = parse_pdf(pdf_content)\n",
    "            # print(\"before preproc\",len(sentences))\n",
    "\n",
    "            for sentence in sentences:\n",
    "                preprocessed_sentence = preprocess_text(sentence)\n",
    "                embedding = generate_embedding(preprocessed_sentence)\n",
    "                all_embeddings.append(embedding) \n",
    "                all_sentences.append(sentence)\n",
    "            # print(\"after preproc\",len(all_sentences))\n",
    "\n",
    "    embeddings_array = np.array(all_embeddings)\n",
    "    return embeddings_array, all_sentences\n",
    "\n",
    "\n",
    "pdf_directory = 'User Assisyance R2024x'\n",
    "corpus_embeddings, corpus_text=process_corpus(pdf_directory)\n",
    "print(\"shape of corpus embeddings\",corpus_embeddings.shape)\n",
    "print(\"len sentences\",len(corpus_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizing or parallelizing parts of this process if necessary, this script may take a while to run cause of the size and number of your PDFs,\n",
    "Storing embeddings efficiently is important, especially in large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS is a library for efficient similarity search and clustering of dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<faiss.swigfaiss_avx2.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x000001FACEF0E990> >\n"
     ]
    }
   ],
   "source": [
    "# FAISS (Facebook AI Similarity Search) is an efficient library for similarity search and clustering of dense vectors\n",
    "def build_faiss_index(embeddings):\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d) #L2 distance (Euclidean distance) to compute similarities between vector\n",
    "    index.add(embeddings)\n",
    "    return index\n",
    "#test with embedding api \n",
    "pdf_directory = 'User Assisyance R2024x'\n",
    "# embeddings_response = get_embeddings(text_to_embed)\n",
    "embeddings_response,_ = process_corpus(pdf_directory)\n",
    "print(build_faiss_index(embeddings_response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc_ids(doc_ids, filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(doc_ids, f)\n",
    "\n",
    "def load_doc_ids(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SearchService:\n",
    "    def __init__(self, embeddings=None, dimension=None, doc_ids=None):\n",
    "        self.doc_ids = []\n",
    "        if embeddings is not None:\n",
    "            d = embeddings.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            self.index.add(embeddings)\n",
    "            if doc_ids is not None:\n",
    "                self.doc_ids = doc_ids\n",
    "        elif dimension is not None:\n",
    "            self.index = faiss.IndexFlatL2(dimension)\n",
    "            if doc_ids is not None:\n",
    "                self.doc_ids = doc_ids\n",
    "        else:\n",
    "            raise ValueError(\"Must provide either embeddings or dimension of the vectors\")\n",
    "\n",
    "    \n",
    "    def search(self, query, k=10):\n",
    "        query_embedding = generate_embedding([query])[0].reshape(1, -1)\n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        if self.doc_ids:\n",
    "            filtered_indices = [self.doc_ids[i] for i in indices[0] if i < len(self.doc_ids)]\n",
    "\n",
    "        else:\n",
    "            filtered_indices = indices[0].tolist()\n",
    "        \n",
    "        distances_returned = distances[0] if distances.size > 0 else []\n",
    "        return filtered_indices, distances_returned\n",
    "\n",
    "    def save_index_and_docs(self, index_name, doc_ids_filename):\n",
    "        faiss.write_index(self.index, index_name)\n",
    "        save_doc_ids(self.doc_ids, doc_ids_filename)\n",
    "\n",
    "    def read_index_and_docs(self, index_name, doc_ids_filename):\n",
    "        self.index = faiss.read_index(index_name)\n",
    "        self.doc_ids = load_doc_ids(doc_ids_filename)\n",
    "\n",
    "    def add_documents(self, new_embeddings, new_doc_ids):\n",
    "        self.index.add(new_embeddings)\n",
    "        self.doc_ids.extend(new_doc_ids)\n",
    "    # def save_doc_ids(doc_ids, filename):\n",
    "    #     with open(filename, 'w') as f:\n",
    "    #         json.dump(doc_ids, f)\n",
    "\n",
    "    # def load_doc_ids(filename):\n",
    "    #     with open(filename, 'r') as f:\n",
    "    #         return json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'index': 0, 'score': 0.7337381, 'text': 'search in topbar'}, {'index': 2, 'score': 0.099820666, 'text': 'Text 3'}, {'index': 1, 'score': 0.08929565, 'text': 'Text 2'}]}\n"
     ]
    }
   ],
   "source": [
    "# def rerank(query, doc_ids, text_chunks):\n",
    "#     api_key = \"vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=\"\n",
    "#     reranking_api_endpoint= \"http://px101.prod.exalead.com:8110/v1/rerank\"\n",
    "\n",
    "#     headers = {\n",
    "#         'Authorization': 'Bearer vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=',\n",
    "#         'Content-Type': 'application/json'\n",
    "#     }\n",
    "#     if not all(doc_id < len(text_chunks) for doc_id in doc_ids):\n",
    "#         raise ValueError(\"One or more doc_ids are out of range for the provided text_chunks.\")\n",
    "\n",
    "#     texts_to_rerank = [text_chunks[i] for i in doc_ids]    \n",
    "#     payload = {\n",
    "#         \"model\": \"BAAI/bge-reranker-large\",  \n",
    "#         \"query\": query,\n",
    "#         \"texts\":  texts_to_rerank,\n",
    "#         \"raw_scores\": False,  \n",
    "#         \"return_text\": True, \n",
    "#         \"truncate\": False, \n",
    "#         \"top_n_result\": 20, \n",
    "#     }\n",
    "#     response = requests.post(reranking_api_endpoint, headers=headers, json=payload)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         reranked_docs = response.json()\n",
    "#         return reranked_docs\n",
    "#     else:\n",
    "#         raise Exception(f\"Failed to rerank documents: {response.status_code} {response.text}\")\n",
    "\n",
    "# # Assuming 'doc_ids' is a list of indices for documents to rerank and 'text_chunks' is a list of document texts\n",
    "# query = \"search\"\n",
    "# doc_ids = [0, 1, 2] \n",
    "# text_chunks = [\"search in topbar\", \"Text 2\", \"Text 3\"] \n",
    "# reranked_docs = rerank(query, doc_ids, text_chunks)\n",
    "# print(reranked_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text generation using the completion endpoint of the model mistral, with the top reranked document content as the prompt. \n",
    "The model will attempt to generate a continuation of this prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create prompt from rerank output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_from_rag_output(output):\n",
    "    # prompt = ' '.join([doc['text'] for doc in output])\n",
    "    prompt = ' '.join(output)\n",
    "\n",
    "    return prompt\n",
    "\n",
    "# rag_output = [\n",
    "#     {'index': 10, 'score': 0.07276838, 'text': '1.'}, \n",
    "#     {'index': 0, 'score': 0.0107767265, 'text': 'To access additional search modes and\\noptions, click \\ue139.'},\n",
    "#     {'index': 12, 'score': 0.007876706, 'text': '2.'}, \n",
    "#     {'index': 2, 'score': 0.0026419468, 'text': 'Note: By default, the search field displays a Search placeholder.'}, {'index': 1, 'score': 0.0025409926, 'text': 'Access 3DSearch by doing one of the following:\\nUse the search field in the 3DEXPERIENCE platform Top Bar.'}, {'index': 5, 'score': 0.0020992735, 'text': 'Accessing the 3DSearch Service\\nYou can access the 3DSearch service from the 3DEXPERIENCE platform Top Bar or the\\n3DSearch dashboard app.'}, {'index': 3, 'score': 0.0013354007, 'text': '4/3/24, 10:34 AM Accessing the 3DSearch Service - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-t-Search-access.htm?contextscope=cloud&id=dc4790a5d0f44c4b… 2/2\\nNote: In the widget \\ue115 Preferences, you can:\\nChange the platform tenant.'}, {'index': 7, 'score': 0.0010987311, 'text': 'Optional: The 6W Search experience is the default search mode.'}, {'index': 4, 'score': 0.0009217804, 'text': 'Open the 3DSearch dashboard app from the Compass.'}, {'index': 6, 'score': 0.0008761799, 'text': '4/3/24, 10:34 AM Accessing the 3DSearch Service - 2024x - dsdoc\\nhttps://help.3ds.com/2024x/english/dsdoc/EXP3DBasicsUserMap/exp3dbasics-t-Search-access.htm?contextscope=cloud&id=dc4790a5d0f44c4b… 1/2\\n© 1995-2024  Dassault Systèmes.'}, {'index': 13, 'score': 0.00076729245, 'text': \"The 3DSearch app is particularly useful if you are using CATIAV5 POWER'BY.\"}, {'index': 11, 'score': 0.00046010586, 'text': 'Once the 3DSearch panel appears, you can dock it anywhere on the frame.'}, {'index': 9, 'score': 0.00039977103, 'text': 'It gives you a quick access\\nto search from the 3DEXPERIENCE platform side panel, since native apps do not display the Top Bar.'}, {'index': 8, 'score': 0.00011235328, 'text': 'It remains pinned on the\\nselected side, even if you open or explore your search results.'}, {'index': 19, 'score': 7.722192e-05, 'text': 'When you select another mode, this\\nplaceholder changes so that you always know which is the current mode.'}]\n",
    "\n",
    "# prompt = create_prompt_from_rag_output(rag_output)\n",
    "# print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is the capital city of France, so the capital of Paris would be Paris itself. It is located in the north-central part of the country and is one of the most populous and culturally significant cities in the world. Paris is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral, as well as its rich history, art, fashion, and cuisine. It is also the largest city in France and the center of the country's political, economic, and cultural life.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(prompt):\n",
    "    url = 'http://px101.prod.exalead.com:8110/v1/chat/completions'\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    payload = {\n",
    "        \"model\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "        \"messages\": messages,  \n",
    "        \"max_tokens\": 1000,\n",
    "        \"top_p\": 1,\n",
    "        \"stop\": [\"string\"],\n",
    "        \"user\": \"string\",\n",
    "        \"response_format\": {\n",
    "            \"type\": \"text\", \n",
    "            \"temperature\": 0.7\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        generated_response = response.json()['choices'][0]['message']['content'].strip()\n",
    "        # generated_response = response.json()\n",
    "        return generated_response\n",
    "    else:\n",
    "        return f\"Failed to generate response. Status code: {response.status_code}\\nResponse: {response.text}\"\n",
    "\n",
    "# Example usage:\n",
    "response = generate_response(\"what is the capital of paris\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #RAG pipeline\n",
    "# def chatbot_response(query):\n",
    "#     # text_pages, _ = parse_pdf(pdf_content, pdf_name)\n",
    "#     # embeddings = get_embeddings(text_pages)\n",
    "#     pdf_directory = 'User Assisyance R2024x'\n",
    "#     corpus_embeddings=process_corpus(pdf_directory)\n",
    "#     index = build_faiss_index(corpus_embeddings)\n",
    "#     doc_ids = search(query, index)\n",
    "#     print(\"doc_ids\",doc_ids)\n",
    "#     texts_for_reranking = [corpus_embeddings[i][0] for i in doc_ids]\n",
    "#     print(texts_for_reranking)\n",
    "#     reranked_docs = rerank(query, doc_ids, texts_for_reranking)\n",
    "#     prompt=create_prompt_from_rag_output(reranked_docs[\"data\"])\n",
    "#     response = generate_response(prompt)\n",
    "#     return response\n",
    "# #test\n",
    "# user_query = \"How to search?\"\n",
    "# response = chatbot_response(user_query)\n",
    "# print(\"response\",response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response : \n",
      " 3DSearch, a search feature in the 3DS software, interprets most non-alphanumeric characters as separators or reserved operators for its User Query Language (UQL). To search for a\n"
     ]
    }
   ],
   "source": [
    "#RAG pipeline\n",
    "def chatbot_response(query):\n",
    "    # text_pages, _ = parse_pdf(pdf_content, pdf_name)\n",
    "    # embeddings = get_embeddings(text_pages)\n",
    "\n",
    "    pdf_directory = 'User Assisyance R2024x'\n",
    "    corpus_embeddings,doc_ids=process_corpus(pdf_directory)\n",
    "\n",
    "    search_service = SearchService(corpus_embeddings,doc_ids=doc_ids)\n",
    "    result = search_service.search(query,20)\n",
    "    search_service.save_index_and_docs(\"my_index\",\"chunks\")\n",
    "   \n",
    "    # search_service = SearchService(None, 512)\n",
    "    # search_service.read_index_and_docs(\"my_index\", \"chunks\")\n",
    "    # result = search_service.search(query,30)\n",
    "\n",
    "    prompt=create_prompt_from_rag_output(result[0])\n",
    "    response = generate_response(prompt)\n",
    "    return response\n",
    "#test\n",
    "# user_query = \"how to search\"\n",
    "# user_query =\"Special Characters\"\n",
    "# user_query =\" History\"\n",
    "# user_query =\" advanced search\"\n",
    "# user_query =\"selecting multiple objects \"\n",
    "user_query =\"special characters\"\n",
    "\n",
    "response = chatbot_response(user_query)\n",
    "print(\"response : \\n\",response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
