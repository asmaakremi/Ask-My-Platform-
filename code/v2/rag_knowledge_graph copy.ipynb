{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, KnowledgeGraphIndex\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\AAI47\\poc2\\flask-api\\code\\rdf\\rdf.txt\"\n",
    "\n",
    "reader = SimpleDirectoryReader(input_files=[file_path])\n",
    "documents = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Optional, List, Mapping, Any\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, SummaryIndex\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "class LLM(CustomLLM):\n",
    "    context_window: int = 3900\n",
    "    num_output: int = 256\n",
    "    model_name: str = \"custom\"\n",
    "    api_url: str = \"http://px101.prod.exalead.com:8110/v1/chat/completions\"\n",
    "    base_prompt = \"Convert this natural language query to a SPARQL query based on the rdf/owl provided. map each element in the NLQ to It correspond ontology/class and relationship with properties defined in the rdf. Please respond only with SparQL Query \"\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    base_prompt: str = \"\"\"\n",
    "You are an expert in transforming natural language queries into UQL queries using a specified ontology. Based on user questions about data stored in an RDF graph, employ the provided ontology, documentation, and steps to understand the UQL syntax. Then, generate the UQL query equivalent for the given natural language query. The response should contain only the UQL query without any explanations or additional text.\n",
    "\n",
    "** UQL Documentation **\n",
    "    UQL queries are expressed in a pseudo UQL format with operators like AND, OR, NOT. Attribute names should be placed in square brackets. Special characters in attribute names need to be escaped with '\\\\\\\\'. \n",
    "    1. **Basic UQL Structure**:\n",
    "        - Attribute names cannot contain characters like .:%#[]$;{{}}.\n",
    "        - A mapping service translates the names exposed to the user and the names used by Cloudview.\n",
    "        - Use square brackets for predicate names. If a predicate is unknown, replace it with #false.\n",
    "        - Escape the first square bracket ‘[’ with ‘\\\\\\\\’ to cancel the attribute name mapping, or use quotes ‘“’ to disable it inside quotes.\n",
    "\n",
    "**Detailed UQL Query Construction Process **\n",
    "    1. **Identify Relevant Classes and Properties**: Review the ontology to determine which classes or properties are relevant to the query.\n",
    "    2. **Map Natural Language to RDF Classes**: Use the ontology to correlate the identified natural language elements with the appropriate RDF classes and predicates.Translate terms like 'physical products' and 'products' directly to their RDF class equivalents based on their definitions or equivalences in the ontology.\n",
    "    3. **Construct the UQL Query**: Formulate the UQL query based on these mappings, ensuring to use only those RDF classes and properties directly relevant or defined as equivalent.\n",
    "\n",
    "**Example UQL Query Construction**\n",
    "    Given the natural language query: \"Show me all documents created 1 juin 2024 , by John Doe\"\n",
    "    - **Step 1: Parse the Natural Language Query**\n",
    "        - Subject: Documents .\n",
    "        - Predicate: Created by and date of creation .\n",
    "        - Object: John Doe and 1 juin 2024 .\n",
    "\n",
    "    - **Step 2: Map to RDF Concepts**\n",
    "        - \"Documents\" corresponds to instances of the 'Document' class.\n",
    "        - \"Created by\" maps to the 'ds6w:lastModifiedBy' or 'ds6w:responsible'.\n",
    "        - \"Created 1 juin 2024 \" maps to 'ds6w:created' .\n",
    "        - \"John Doe\" corresponds to instances of the 'Person' class\n",
    "\n",
    "    - **Step 3: Formulate the UQL Query**\n",
    "        -[ds6w:created]>=\\\"2024-06-01T00:00:00.000Z\\\" AND [ds6w:created]<=\\\"2024-06-01T23:59:59.000Z\\\" AND [ds6w:type]:\\\"Document\\\" AND (([ds6w:lastModifiedBy]:\\\"John Doe\\\" OR [ds6w:responsible]:\\\"John Doe\\\")\n",
    "    \n",
    "    ** Example UQL Queries **\n",
    "   - Example 1:\n",
    "        - Natural Language: give me products that are created between 2024-05-01 to 2024-05-28 by : MCM OCDxComplianceUser\n",
    "        - UQL: [ds6w:created]>=\"2024-05-01T00:00:00.000Z\" AND [ds6w:created]<=\"2024-05-28T23:59:59.000Z\" AND [ds6w:type]:\"VPMReference\"  AND (([ds6w:lastModifiedBy]:\"MCM OCDxComplianceUser\" OR [ds6w:responsible]:\"MCM OCDxComplianceUser\") \n",
    "\n",
    "Based on the natural language query \"{query}\" and the current date if needed, generate the corresponding UQL query using the ontology and RDF relationships. Ensure the output strictly adheres to the syntax and ontology requirements without adding or assuming types not explicitly defined.\n",
    "\n",
    "Please respond ONLY with the valid UQL query.\n",
    "\"\"\"\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=self.context_window,\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self,prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        full_prompt = self.base_prompt + prompt\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "        payload = {\n",
    "            \"model\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 1000,\n",
    "            \"top_p\": 1,\n",
    "            \"stop\": [\"string\"],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"text\", \n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            generated_response = response.json()['choices'][0]['message']['content'].strip()\n",
    "            return CompletionResponse(text=generated_response)\n",
    "        else:\n",
    "            return CompletionResponse(text=\"Error: API request failed\")\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(\n",
    "        self, prompt: str, **kwargs: Any\n",
    "    ) -> CompletionResponseGen:\n",
    "        full_prompt = self.base_prompt + prompt\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "        payload = {\n",
    "            \"model\":\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            \"messages\": messages,\n",
    "            \"max_tokens\": 1500,\n",
    "            \"top_p\": 1,\n",
    "            \"stop\": [\"string\"],\n",
    "            \"response_format\": {\n",
    "                \"type\": \"text\", \n",
    "                \"temperature\": 0.7\n",
    "            }\n",
    "        }\n",
    "        response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            generated_response = response.json()['choices'][0]['message']['content'].strip()\n",
    "            for token in generated_response:\n",
    "                yield CompletionResponse(text=token, delta=token)\n",
    "        else:\n",
    "            yield CompletionResponse(text=\"Error\", delta=\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "from llama_index.core.embeddings import BaseEmbedding\n",
    "import requests\n",
    "\n",
    "class CustomAPIEmbeddings(BaseEmbedding):\n",
    "    _api_key: str = \"vtYvpB9U+iUQwl0K0MZIj+Uo5u6kilAZJdgHGVBEhNc=\"\n",
    "    _embeddings_url: str = \"http://px101.prod.exalead.com:8110/v1/embeddings\"\n",
    "    _headers = {\n",
    "        'Authorization': 'Bearer ' + _api_key,\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def class_name(cls) -> str:\n",
    "        return \"custom_api\"\n",
    "\n",
    "    def _aget_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_embeddings([query], \"Represent a document for semantic search:\")[0]\n",
    "\n",
    "    def _get_query_embedding(self, query: str) -> List[float]:\n",
    "        return self._get_embeddings([query], \"Represent a document for semantic search:\")[0]\n",
    "\n",
    "    def _get_text_embedding(self, text: str) -> List[float]:\n",
    "        return self._get_embeddings([text], \"Represent a document for semantic search:\")[0]\n",
    "\n",
    "    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:\n",
    "        return self._get_embeddings(texts, \"Represent a document for semantic search:\")\n",
    "\n",
    "    def _get_embeddings(self, texts: List[str], instruction: str) -> List[List[float]]:\n",
    "        payload = {\n",
    "            \"model\": \"BAAI/bge-large-en-v1.5\",\n",
    "            \"input\": texts,\n",
    "            \"encoding_format\": \"float\",\n",
    "            \"instruct\": instruction,\n",
    "        }\n",
    "        response = requests.post(self._embeddings_url, headers=self._headers, json=payload)\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            embeddings_list = [item['embedding'] for item in response_data['data']]\n",
    "            return np.array(embeddings_list).tolist()  \n",
    "        else:\n",
    "            raise Exception(f\"Failed to get embeddings: {response.status_code}, {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = LLM()\n",
    "Settings.chunk_size = 512\n",
    "embed_model = CustomAPIEmbeddings(embed_batch_size=2)\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext\n",
    "\n",
    "graph_store = SimpleGraphStore()\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "index =KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    max_triplets_per_chunk=20,\n",
    "    include_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KnowledgeGraphIndex' object has no attribute 'get_chunks'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chunks\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KnowledgeGraphIndex' object has no attribute 'get_chunks'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "chunks = index.get_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Owl:class]:\"Pno:person\"\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    include_text=False, response_mode=\"tree_summarize\"\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"give me  type persons\",\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
